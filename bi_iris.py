# -*- coding: utf-8 -*-
"""BI_IRIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S0CI-6QWzOlAFmXn46H1nRxTbnVmONaP
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.preprocessing import StandardScaler

df = pd.read_csv("Iris.csv")

print(f"The number of Rows : {df.shape[0]}\nThe number of columns : {df.shape[1]}")

"""**DATA ANALYSIS**"""

df.info()

df.isnull().sum()

df.value_counts('Species')

plt.figure(figsize = (5,4))
sns.countplot(x = 'Species', data = df, palette = 'Accent')
plt.title('Distibution of Species');

plt.figure(figsize=(14,6))
df['Species'].value_counts().plot(kind='pie',
                                  autopct='%0.2f%%',ylabel='',
                                  title='Species');

df['Species'].nunique()

setosa = df[df.Species == 'Iris-setosa']
versicolor = df[df.Species == 'Iris-versicolor']
virginica = df[df.Species == 'Iris-virginica']

setosa.describe()

versicolor.describe()

virginica.describe()

sns.boxplot(x = 'Species', y = 'SepalLengthCm', data = df);

sns.boxplot(x = 'Species', y = 'SepalWidthCm', data = df);

sns.boxplot(x = 'Species', y = 'PetalLengthCm', data = df);

sns.boxplot(x = 'Species', y = 'PetalWidthCm', data = df);

sns.heatmap(df.iloc[:,:-1].corr(), annot = True, cmap = 'YlGnBu');

sns.heatmap(setosa.iloc[:,:-1].corr(), annot = True, cmap = 'YlGnBu')
plt.title('Setosa Features Correlation');

sns.heatmap(versicolor.iloc[:,:-1].corr(), annot = True, cmap = 'YlGnBu')
plt.title('Versicolor Features Correlation');

sns.heatmap(virginica.iloc[:,:-1].corr(), annot = True, cmap = 'YlGnBu')
plt.title('Virginica Features Correlation');

"""**Encoding**"""

def encode_species(s):
    if s == 'Iris-setosa':
        return 0
    elif s == 'Iris-versicolor':
        return 1
    else :  #Iris-virginica
        return 2

def decode_species(s):
    if s == 0:
        return 'Iris-setosa'
    elif s == 1:
        return 'Iris-versicolor'
    else : #s == 2
        return 'Iris-virginica'

df['Species'] = df['Species'].apply(lambda x : encode_species(x))

#converting datatype to integer
df['Species'] = df['Species'].astype('int32')

df.value_counts('Species')

x = df.drop(['Id','Species'], axis = 1).values
y = df.iloc[:,-1].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25,
                                                    random_state= 42, stratify = y)

print(f"""SHAPES :
x_train : {x_train.shape}
y_train : {y_train.shape}
x_test : {x_test.shape}
y_test : {y_test.shape}""")

"""Model Building
Classification Models :

Logistic Regression
K Nearest Neighbors
Naive Bayes
Decision Tree
Support Vector Machine
Random Forest
"""

logistic_regression = LogisticRegression()
logistic_regression.fit(x_train, y_train)
logistic_regression_predictions = logistic_regression.predict(x_test)

def get_report(actual, predicted):

    print(f"ACCURACY : {accuracy_score(actual, predicted)}")

    print(f"CLASSIFICATION REPORT : \n{classification_report(actual, predicted)}")

    conf_mat  = confusion_matrix(actual, predicted)

    sns.heatmap(conf_mat, annot = True, cmap = 'Purples')
    plt.title('CONUSION MATRIX');

get_report(y_test, logistic_regression_predictions)



k_nearest = KNeighborsClassifier()
k_nearest.fit(x_train, y_train)

k_nearest_predictions = k_nearest.predict(x_test)
get_report(y_test, k_nearest_predictions)



naive_bayes = GaussianNB()
naive_bayes.fit(x_train, y_train)

naive_bayes_predictions = naive_bayes.predict(x_test)
get_report(y_test, naive_bayes_predictions)



decision_tree = DecisionTreeClassifier()
decision_tree.fit(x_train, y_train)

decision_tree_predictions = decision_tree.predict(x_test)

get_report(y_test, decision_tree_predictions)



support_vec = SVC()
support_vec.fit(x_train, y_train)

support_vec_predictions = support_vec.predict(x_test)

get_report(y_test, support_vec_predictions)



random_forest = RandomForestClassifier()
random_forest.fit(x_train, y_train)

random_forest_predictions = random_forest.predict(x_test)

get_report(y_test, random_forest_predictions)



results = []

for i in k_nearest_predictions:
    results.append(decode_species(i))

sns.histplot(results, palette = 'Accent')
plt.title('Distribution of Results');

def get_result(sepal_length, sepal_width, petal_length, petal_width ):

    data = np.array([sepal_length, sepal_width, petal_length, petal_width])
    data = data.reshape(1, -1)
    prediction = k_nearest.predict(data)

    return decode_species(prediction[0])

sepal_length = 1.2
sepal_width = 3.2
petal_length = 4.5
petal_width = 5

get_result(sepal_length, sepal_width, petal_length, petal_width)